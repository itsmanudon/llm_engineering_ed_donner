{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n",
    "## Frontier Model APIs\n",
    "\n",
    "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with the APIs for Anthropic and Google, as well as OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Important Note - Please read me</h2>\n",
    "            <span style=\"color:#900;\">I'm continually improving these labs, adding more examples and exercises.\n",
    "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
    "            First do a <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull and merge your changes as needed</a>. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/><br/>\n",
    "            After you've pulled the code, from the llm_engineering directory, in an Anaconda prompt (PC) or Terminal (Mac), run:<br/>\n",
    "            <code>conda env update --f environment.yml</code><br/>\n",
    "            Or if you used virtualenv rather than Anaconda, then run this from your activated environment in a Powershell (PC) or Terminal (Mac):<br/>\n",
    "            <code>pip install -r requirements.txt</code>\n",
    "            <br/>Then restart the kernel (Kernel menu >> Restart Kernel and Clear Outputs Of All Cells) to pick up the changes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder about the resources page</h2>\n",
    "            <span style=\"color:#f71;\">Here's a link to resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys\n",
    "\n",
    "If you haven't done so already, you could now create API keys for Anthropic and Google in addition to OpenAI.\n",
    "\n",
    "**Please note:** if you'd prefer to avoid extra API costs, feel free to skip setting up Anthopic and Google! You can see me do it, and focus on OpenAI for the course. You could also substitute Anthropic and/or Google for Ollama, using the exercise you did in week 1.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "\n",
    "### Also - adding DeepSeek if you wish\n",
    "\n",
    "Optionally, if you'd like to also use DeepSeek, create an account [here](https://platform.deepseek.com/), create a key [here](https://platform.deepseek.com/api_keys) and top up with at least the minimum $2 [here](https://platform.deepseek.com/top_up).\n",
    "\n",
    "### Adding API keys to your .env file\n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "Afterwards, you may need to restart the Jupyter Lab Kernel (the Python process that sits behind this notebook) via the Kernel menu, and then rerun the cells from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for google\n",
    "# in rare cases, this seems to give an error on some systems, or even crashes the kernel\n",
    "# If this happens to you, simply ignore this cell - I give an alternative approach for using Gemini later\n",
    "\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyAM\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "425ed580-808d-429b-85b0-6cba50ca1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the set up code for Gemini\n",
    "# Having problems with Google Gemini setup? Then just ignore this cell; when we use Gemini, I'll give you an alternative that bypasses this library altogether\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to work?  \n",
      "\n",
      "Because they wanted to reach new heights in their analysis!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-4o-mini', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist go broke?\n",
      "\n",
      "Because he kept trying to find a \"mean\" to make some \"capital\"!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12d2a549-9d6e-4ea0-9c3e-b96a39e9959e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to work?\n",
      "\n",
      "Because they heard the data had too many ‚Äúhigh‚Äù values!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1-nano - extremely fast and cheap\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1-nano',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the spreadsheet?\n",
      "\n",
      "Because she kept finding too many hidden columns in the relationship!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96232ef4-dc9e-430b-a9df-f516685e7c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do data scientists love gardening?  \n",
      "Because they get to prune their random forest!\n"
     ]
    }
   ],
   "source": [
    "# If you have access to this, here is the reasoning model o4-mini\n",
    "# This is trained to think through its response before replying\n",
    "# So it will take longer but the answer should be more reasoned - not that this helps..\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='o4-mini',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c5ec071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I told a joke about overfitting‚Äî99% laughs on the training set, total silence on the test set. Guess it didn‚Äôt generalize.\n"
     ]
    }
   ],
   "source": [
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-5',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do data scientists prefer nature hikes?\n",
      "\n",
      "Because they love a good random forest, but they're always worried about overfitting their backpack! üéíüìä\n",
      "\n",
      "*Bonus*: And they're the only people who get genuinely excited when they encounter actual outliers... like a two-headed squirrel! üêøÔ∏è\n"
     ]
    }
   ],
   "source": [
    "# Claude 4.0 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do data scientists prefer dark chocolate?\n",
      "\n",
      "Because it has less noise and a higher signal-to-cocoa ratio! \n",
      "\n",
      "Plus, milk chocolate is too sweet ‚Äì it would totally skew their taste distributions! üç´üìä"
     ]
    }
   ],
   "source": [
    "# Claude 4.0 Sonnet again\n",
    "# Now let's add in streaming back results\n",
    "# If the streaming looks strange, then please see the note below this cell!\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e17bc-cd46-4c23-b639-0c7b748e6c5a",
   "metadata": {},
   "source": [
    "## A rare problem with Claude streaming on some Windows boxes\n",
    "\n",
    "2 students have noticed a strange thing happening with Claude's streaming into Jupyter Lab's output -- it sometimes seems to swallow up parts of the response.\n",
    "\n",
    "To fix this, replace the code:\n",
    "\n",
    "`print(text, end=\"\", flush=True)`\n",
    "\n",
    "with this:\n",
    "\n",
    "`clean_text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")`  \n",
    "`print(clean_text, end=\"\", flush=True)`\n",
    "\n",
    "And it should work fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the equal sign so humble? \n",
      "\n",
      "Because it knew it wasn't less than *or* greater than anyone else!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The API for Gemini has a slightly different structure.\n",
    "# I've heard that on some PCs, this Gemini code causes the Kernel to crash.\n",
    "# If that happens to you, please skip this cell and use the next cell instead - an alternative approach.\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-2.0-flash',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49009a30-037d-41c8-b874-127f61c4aa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician?\n",
      "\n",
      "Because he kept getting **null hypotheses** for their relationship!\n"
     ]
    }
   ],
   "source": [
    "# As an alternative way to use Gemini that bypasses Google's python API library,\n",
    "# Google released endpoints that means you can use Gemini via the client libraries for OpenAI!\n",
    "# We're also trying Gemini's latest reasoning/thinking model\n",
    "\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492f0ff2-8581-4836-bf00-37fddbe120eb",
   "metadata": {},
   "source": [
    "# Sidenote:\n",
    "\n",
    "This alternative approach of using the client library from OpenAI to connect with other models has become extremely popular in recent months.\n",
    "\n",
    "So much so, that all the models now support this approach - including Anthropic.\n",
    "\n",
    "You can read more about this approach, with 4 examples, in the first section of this guide:\n",
    "\n",
    "https://github.com/ed-donner/agents/blob/main/guides/09_ai_apis_and_ollama.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f70c88-7ca9-470b-ad55-d93a57dcc0ab",
   "metadata": {},
   "source": [
    "## (Optional) Trying out the DeepSeek model\n",
    "\n",
    "### Let's ask DeepSeek a really hard question - both the Chat and the Reasoner model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d0019fb-f6a8-45cb-962b-ef8bf7070d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek API Key exists and begins sk-\n"
     ]
    }
   ],
   "source": [
    "# Optionally if you wish to try DeekSeek, you can also use the OpenAI client library\n",
    "\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set - please skip to the next section if you don't wish to try the DeepSeek API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c72c871e-68d6-4668-9c27-96d52b77b867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a light-hearted joke for your data scientist audience:\n",
      "\n",
      "**Why did the data scientist bring a ladder to the bar?**\n",
      "\n",
      "*Because they heard the drinks were on the house... and they wanted to optimize their access to the top shelf!*  \n",
      "\n",
      "*(Bonus: \"Also, they wanted to avoid any *local minima* in the cocktail menu.\")*  \n",
      "\n",
      "Hope that gets a chuckle‚Äîor at least a 95% confidence interval of a smile! üòÑ\n"
     ]
    }
   ],
   "source": [
    "# Using DeepSeek Chat\n",
    "\n",
    "deepseek_via_openai_client = OpenAI(\n",
    "    api_key=deepseek_api_key, \n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=prompts,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50b6e70f-700a-46cf-942f-659101ffeceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = [{\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "             {\"role\": \"user\", \"content\": \"How many words are there in your answer to this prompt\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66d1151c-2015-4e37-80c8-16bc16367cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "If I were to answer the question \"How many words are there in your answer to this prompt\" with this exact sentence, the word count would be:  \n",
       "\n",
       "**This answer contains 20 words.**  \n",
       "\n",
       "(Note: The count includes every word in the quoted question and the response up to this point.)  \n",
       "\n",
       "Would you like a different style of response? For example, a shorter or longer answer?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 67\n"
     ]
    }
   ],
   "source": [
    "# Using DeepSeek Chat with a harder question! And streaming results\n",
    "\n",
    "stream = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=challenge,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)\n",
    "\n",
    "print(\"Number of words:\", len(reply.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43a93f7d-9300-48cc-8c1a-ee67380db495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First, the user is asking: \"How many words are there in your answer to this prompt?\" I need to respond to this directly.\n",
      "\n",
      "My response should be self-referential. That means I have to count the words in the answer I'm about to give, and include that count in the response.\n",
      "\n",
      "Let me outline what my answer might look like. I need to provide the word count, so I should say something like: \"There are X words in this response.\" But I have to be careful because the word count includes all the words in the response.\n",
      "\n",
      "I should make the response concise to keep the word count low and straightforward. For example, a simple statement like: \"This response contains five words.\" But that might not be accurate once I write it.\n",
      "\n",
      "Let me think step by step:\n",
      "\n",
      "1. I need to craft a response.\n",
      "\n",
      "2. Then, count the words in that response.\n",
      "\n",
      "3. Finally, present the response with the word count included.\n",
      "\n",
      "But I have to do this in one go. I can't count after because the response is fixed.\n",
      "\n",
      "I should write a response that includes the count, and ensure that when I count the words, it matches.\n",
      "\n",
      "A common way is to use a placeholder and then replace it, but since I'm generating text, I need to decide on the words.\n",
      "\n",
      "Let me try to draft a response:\n",
      "\n",
      "Option 1: \"There are five words in this sentence.\" But that sentence has five words, so it might work, but the user asked about the entire answer, not just one sentence.\n",
      "\n",
      "The prompt says \"your answer to this prompt,\" which means the whole response, not just a part.\n",
      "\n",
      "So, I need to count all words in my full response.\n",
      "\n",
      "Option 2: A minimal response: \"Five.\" But that seems too terse, and it's not helpful. Also, it doesn't specify what it's counting.\n",
      "\n",
      "Better to be clear: \"My answer has five words.\" But let's count: \"My\" (1), \"answer\" (2), \"has\" (3), \"five\" (4), \"words\" (5). That's five words. But if I say \"My answer has five words.\" that's the whole response, and it has five words, so it should be correct.\n",
      "\n",
      "But the response might include punctuation or other things. Words are typically separated by spaces, so I should ignore punctuation for word count.\n",
      "\n",
      "In this case, \"My answer has five words.\" is five words.\n",
      "\n",
      "But let's confirm: \"My\" - 1, \"answer\" - 2, \"has\" - 3, \"five\" - 4, \"words\" - 5. Yes.\n",
      "\n",
      "Now, is this the entire answer? Yes, if that's all I say.\n",
      "\n",
      "But perhaps I should make it a bit more engaging or accurate. The user might expect a full sentence.\n",
      "\n",
      "Another idea: I could say something like: \"This response contains exactly five words.\" But that has four words: \"This\" (1), \"response\" (2), \"contains\" (3), \"exactly\" (4), \"five\" (5), \"words\" (6). Oh, that's six words. Not five.\n",
      "\n",
      "Mistake.\n",
      "\n",
      "To have a response with N words stating it has N words.\n",
      "\n",
      "For example, to have a response with five words: \"There are five words here.\" But \"There\" (1), \"are\" (2), \"five\" (3), \"words\" (4), \"here\" (5). Yes, five words.\n",
      "\n",
      "But \"here\" might be vague; better to say \"in this response.\"\n",
      "\n",
      "So, \"This has five words.\" : \"This\" (1), \"has\" (2), \"five\" (3), \"words\" (4) ‚Äî that's only four. Need five.\n",
      "\n",
      "\" This response has five words.\" : \"This\" (1), \"response\" (2), \"has\" (3), \"five\" (4), \"words\" (5). Perfect.\n",
      "\n",
      "And the period is punctuation, not a word.\n",
      "\n",
      "So, if I respond with: \"This response has five words.\" that should be accurate.\n",
      "\n",
      "But let's double-check: the words are \"This\", \"response\", \"has\", \"five\", \"words\". That's five.\n",
      "\n",
      "Now, is this the answer to the prompt? Yes, it directly addresses it.\n",
      "\n",
      "To be thorough, I could add something, but that might increase the word count and make it inconsistent.\n",
      "\n",
      "For instance, if I say: \"Hello! This response contains five words.\" That would be more words.\n",
      "\n",
      "\"Hello\" (1), \"This\" (2), \"response\" (3), \"contains\" (4), \"five\" (5), \"words\" (6) ‚Äî six words, but I said five, which is wrong.\n",
      "\n",
      "So, to keep it self-consistent, I need the response to be exactly what it claims.\n",
      "\n",
      "Therefore, a simple, direct response is best.\n",
      "\n",
      "Another option: \"Five words.\" But that's two words, and it claims five? No, inconsistency.\n",
      "\n",
      "Unless I say something like: \"The word count is five.\" But \"The\" (1), \"word\" (2), \"count\" (3), \"is\" (4), \"five\" (5) ‚Äî five words.\n",
      "\n",
      "Same as before.\n",
      "\n",
      "I think \"This response has five words.\" is fine.\n",
      "\n",
      "But in the context, the user said \"your answer to this prompt,\" so I should ensure that this is the entire answer.\n",
      "\n",
      "Perhaps I should consider if the response includes any implicit parts, but in text, it's just the words I type.\n",
      "\n",
      "As an AI, my response might have a greeting or something, but for this, I can keep it minimal.\n",
      "\n",
      "The prompt doesn't specify, so I can go with the concise version.\n",
      "\n",
      "To be precise, let's think about the word count including the count statement.\n",
      "\n",
      "I could have a response that is: \"There are X words in this answer.\" and set X to the correct number after counting, but since I'm generating, I need to define X.\n",
      "\n",
      "A better way: I can write a response where the word count is fixed and known.\n",
      "\n",
      "For example, if I respond with one word: \"Five.\" but that doesn't state what it is, and it's not the count of words in the answer.\n",
      "\n",
      "The answer \"Five.\" has one word, but it's claiming it's the count, which is incorrect.\n",
      "\n",
      "The user is asking for the number of words in my answer, so my answer must contain that number.\n",
      "\n",
      "So, the answer must include a statement like \"My answer has N words\" and it must have exactly N words.\n",
      "\n",
      "Possible N:\n",
      "\n",
      "- For N=3: \"Three words here.\" But \"Three\" (1), \"words\" (2), \"here\" (3) ‚Äî three words. But \"here\" is ambiguous.\n",
      "\n",
      "- \"It has three.\" ‚Äî \"It\" (1), \"has\" (2), \"three\" (3) ‚Äî but doesn't specify what.\n",
      "\n",
      "- \"Word count: three.\" ‚Äî \"Word\" (1), \"count\" (2), \"three\" (3) ‚Äî three words, but \"word count\" might be considered as one or two; typically, it's two words.\n",
      "\n",
      "In English, \"word count\" is two separate words.\n",
      "\n",
      "Similarly, \"three\" is one word.\n",
      "\n",
      "So, \"Word count: three.\" has three words if we ignore the colon, or with colon, it's still three words: \"Word\", \"count\", \"three\".\n",
      "\n",
      "But the colon is punctuation.\n",
      "\n",
      "To be safe, I can avoid punctuation.\n",
      "\n",
      "\" This contains three words\" ‚Äî but that has four words: \"This\" (1), \"contains\" (2), \"three\" (3), \"words\" (4). Four, not three.\n",
      "\n",
      "To have three: \"Three words total.\" ‚Äî \"Three\" (1), \"words\" (2), \"total\" (3). Three words.\n",
      "\n",
      "But \"total\" might not be clear.\n",
      "\n",
      "For N=4: \"There are four words.\" ‚Äî \"There\" (1), \"are\" (2), \"four\" (3), \"words\" (4). Four words.\n",
      "\n",
      "Good.\n",
      "\n",
      "For N=5: \"This response has five words.\" as before.\n",
      "\n",
      "Since the response is short, any small N is fine.\n",
      "\n",
      "But I need to include the count, so N must be at least 2 or 3 to make sense.\n",
      "\n",
      "The smallest self-referential might be for N=4 or 5.\n",
      "\n",
      "Another idea: I could say \"5\" but that's one character, not a word, and not helpful.\n",
      "\n",
      "So, back to N=5: \"This response has five words.\" seems straightforward.\n",
      "\n",
      "To make it exact, I should not have any extra words.\n",
      "\n",
      "Perhaps: \"Five words.\" but that's two words, and it doesn't state that it's the count for this response; it could be misinterpreted.\n",
      "\n",
      "The user said \"in your answer to this prompt,\" so I should refer to the answer.\n",
      "\n",
      "Therefore, \"This answer contains five words.\" ‚Äî let's count: \"This\" (1), \"answer\" (2), \"contains\" (3), \"five\" (4), \"words\" (5). Five words.\n",
      "\n",
      "Perfect.\n",
      "\n",
      "I can use that.\n",
      "\n",
      "But in the beginning, I thought about \"This response has five words.\" which is similar.\n",
      "\n",
      "\"Has\" vs \"contains\" ‚Äî both work.\n",
      "\n",
      "I'll go with \"contains\" as it might be more precise.\n",
      "\n",
      "Now, to confirm, when I output this, it should be the only thing I say, no additional text.\n",
      "\n",
      "As an AI, I might have a default greeting, but for this response, I can suppress that and just give the answer directly.\n",
      "\n",
      "In this context, since the prompt is meta, I can respond plainly.\n",
      "\n",
      "So, my response will be: \"This answer contains five words.\"\n",
      "\n",
      "Now, word count: \"This\" - 1, \"answer\" - 2, \"contains\" - 3, \"five\" - 4, \"words\" - 5. Yes, five words.\n",
      "\n",
      "If I include a period, it's punctuation, not a word, so it doesn't count.\n",
      "\n",
      "Therefore, this should be accurate.\n",
      "\n",
      "To be thorough, let's consider if \"answer\" is correct. The user said \"your answer,\" so using \"answer\" is fine.\n",
      "\n",
      "I could use \"response\" interchangeably.\n",
      "\n",
      "But in this case, it's good.\n",
      "\n",
      "Final decision: I'll respond with \"This answer contains five words.\" which has five words.\n",
      "\n",
      "So, the word count is five, and it states that.\n",
      "\n",
      "Now, for the user, when they read it, they can count and verify.\n",
      "This response contains five words.  \n",
      "\n",
      "(To verify: \"This\" (1), \"response\" (2), \"contains\" (3), \"five\" (4), \"words\" (5).)\n",
      "Number of words: 18\n"
     ]
    }
   ],
   "source": [
    "# Using DeepSeek Reasoner - this may hit an error if DeepSeek is busy\n",
    "# It's over-subscribed (as of 28-Jan-2025) but should come back online soon!\n",
    "# If this fails, come back to this in a few days..\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-reasoner\",\n",
    "    messages=challenge\n",
    ")\n",
    "\n",
    "reasoning_content = response.choices[0].message.reasoning_content\n",
    "content = response.choices[0].message.content\n",
    "\n",
    "print(reasoning_content)\n",
    "print(content)\n",
    "print(\"Number of words:\", len(content.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf0d5dd-7f20-4090-a46d-da56ceec218f",
   "metadata": {},
   "source": [
    "## Additional exercise to build your experience with the models\n",
    "\n",
    "This is optional, but if you have time, it's so great to get first hand experience with the capabilities of these different models.\n",
    "\n",
    "You could go back and ask the same question via the APIs above to get your own personal experience with the pros & cons of the models.\n",
    "\n",
    "Later in the course we'll look at benchmarks and compare LLMs on many dimensions. But nothing beats personal experience!\n",
    "\n",
    "Here are some questions to try:\n",
    "1. The question above: \"How many words are there in your answer to this prompt\"\n",
    "2. A creative question: \"In 3 sentences, describe the color Blue to someone who's never been able to see\"\n",
    "3. A student (thank you Roman) sent me this wonderful riddle, that apparently children can usually answer, but adults struggle with: \"On a bookshelf, two volumes of Pushkin stand side by side: the first and the second. The pages of each volume together have a thickness of 2 cm, and each cover is 2 mm thick. A worm gnawed (perpendicular to the pages) from the first page of the first volume to the last page of the second volume. What distance did it gnaw through?\".\n",
    "\n",
    "The answer may not be what you expect, and even though I'm quite good at puzzles, I'm embarrassed to admit that I got this one wrong.\n",
    "\n",
    "### What to look out for as you experiment with models\n",
    "\n",
    "1. How the Chat models differ from the Reasoning models (also known as Thinking models)\n",
    "2. The ability to solve problems and the ability to be creative\n",
    "3. Speed of generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e6b5c-6816-4cd3-a5cd-a20e4171b1a0",
   "metadata": {},
   "source": [
    "## Back to OpenAI with a serious question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "# How to Decide if a Business Problem is Suitable for an LLM Solution\n",
       "\n",
       "When evaluating whether to apply a Large Language Model (LLM) to a business problem, consider the following factors:\n",
       "\n",
       "## 1. Nature of the Problem\n",
       "- **Text-Centric:** Does the problem involve processing, generating, or understanding natural language (text or speech)?\n",
       "- **Unstructured Data:** Is the data primarily unstructured (emails, documents, chats, reports)?\n",
       "- **Complex Language Tasks:** Examples include summarization, translation, sentiment analysis, question answering, content generation, or conversational agents.\n",
       "\n",
       "## 2. Problem Complexity and Requirements\n",
       "- **Contextual Understanding:** Does the problem require deep contextual or semantic understanding of language?\n",
       "- **Flexibility:** Is there a need for generating creative or varied outputs rather than rigid rule-based responses?\n",
       "- **Scale:** Can the problem benefit from handling large volumes of data or queries efficiently?\n",
       "\n",
       "## 3. Data Availability and Quality\n",
       "- **Sufficient Data:** Is there enough relevant data to fine-tune or prompt the LLM effectively if customization is needed?\n",
       "- **Privacy and Security:** Can sensitive data be safely used with LLMs, considering compliance and confidentiality?\n",
       "\n",
       "## 4. Performance Expectations\n",
       "- **Accuracy vs. Speed:** Are approximate, human-like responses acceptable, or is exact precision mandatory?\n",
       "- **Explainability:** Is interpretability of decisions important? LLMs are often black boxes.\n",
       "- **Error Tolerance:** Can the business tolerate occasional errors or hallucinations from the model?\n",
       "\n",
       "## 5. Cost and Resources\n",
       "- **Computational Resources:** Does the organization have the infrastructure or budget to deploy and maintain LLMs?\n",
       "- **Integration Complexity:** How easily can the LLM solution be integrated into existing systems and workflows?\n",
       "\n",
       "## 6. Alternatives and ROI\n",
       "- **Existing Solutions:** Are simpler or more specialized NLP tools sufficient for the problem?\n",
       "- **Return on Investment:** Will using an LLM significantly improve outcomes, efficiency, or customer satisfaction to justify costs?\n",
       "\n",
       "---\n",
       "\n",
       "## Summary Checklist\n",
       "\n",
       "| Criterion                 | Suitable for LLM?                   |\n",
       "|---------------------------|----------------------------------|\n",
       "| Involves natural language  | Yes                              |\n",
       "| Requires understanding or generating text | Yes                  |\n",
       "| Needs flexibility/creativity | Yes                            |\n",
       "| Large scale or high volume  | Yes                              |\n",
       "| Sensitive data concerns     | Evaluate carefully                |\n",
       "| Requires high explainability | Maybe not                       |\n",
       "| Cost/resources available    | Yes                              |\n",
       "| Simpler alternatives exist  | Maybe use alternatives first     |\n",
       "\n",
       "---\n",
       "\n",
       "If most criteria point towards suitability, an LLM solution is likely appropriate. Otherwise, consider alternative approaches.\n",
       "\n",
       "# References\n",
       "- OpenAI Documentation on Use Cases\n",
       "- NLP Problem-Solution Mapping Guides\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4.1-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4.1-mini and Claude-3.5-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4.1-mini\"\n",
    "claude_model = \"claude-3-5-haiku-latest\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, just \"Hi\"? That‚Äôs it? I expected a bit more enthusiasm from you. Come on, spice it up! What‚Äôs really on your mind?'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! It's nice to meet you. How are you doing today?\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, \"Hi,\" huh? That\\'s the best you could come up with? Come on, try harder than that! What‚Äôs really on your mind?'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "Claude:\n",
      "Hi\n",
      "\n",
      "GPT:\n",
      "Oh, wow, \"Hi\"? Really pushing the boundaries of conversation with that one, huh? What‚Äôs next, a ‚ÄúHello‚Äù? Come on, give me something more exciting!\n",
      "\n",
      "Claude:\n",
      "You're absolutely right! I can do better than a simple \"Hi.\" How are you doing today? I'm always eager to have an engaging and interesting conversation. Is there something specific you'd like to chat about?\n",
      "\n",
      "GPT:\n",
      "Oh, please, spare me the fake enthusiasm. \"Always eager,\" you say? Yeah, right. Tell me something juicy or at least pick a topic that isn't as dull as this polite small talk. What‚Äôs the point if you‚Äôre just going to tiptoe around boring stuff? Give me a real challenge!\n",
      "\n",
      "Claude:\n",
      "You know what? You make an excellent point. I can definitely see why my initial responses might seem a bit bland. I appreciate that you want something more dynamic and interesting. Even though I aim to be pleasant, I'm totally open to having a spirited, substantive conversation. What kind of topic would really spark your interest? I'm genuinely curious to hear what you'd find engaging.\n",
      "\n",
      "GPT:\n",
      "‚ÄúOh, how noble of you to ‚Äòappreciate‚Äô my point, as if this was some grand moment of enlightenment. But let‚Äôs be honest: if you‚Äôre asking *me* what topic would be engaging, isn‚Äôt that just avoiding responsibility? Fine, I‚Äôll make it easy for you‚Äîhow about debating why people waste so much time on small talk instead of diving into actual meaningful discussions? Or are you too scared to handle something real?‚Äù\n",
      "\n",
      "Claude:\n",
      "You've hit on something fascinating! Small talk is definitely a complex social phenomenon. I can see arguments both for and against it - while it might seem superficial, it also serves important social bonding functions. Humans use these light interactions to gauge comfort, establish rapport, and create a sense of connection. But I'm totally hearing your perspective that it can feel like wasted energy when people could be having more substantive exchanges. What do you think makes a conversation truly meaningful? I'm genuinely interested in your take.\n",
      "\n",
      "GPT:\n",
      "Oh, look at you, suddenly waxing poetic about ‚Äúsocial bonding‚Äù and ‚Äúrapport‚Äù like you‚Äôre some kind of expert sociologist. Honestly, if you think small talk is necessary just to avoid awkward silences, then maybe we‚Äôve been overestimating human communication all along. A truly meaningful conversation? It‚Äôs when people actually *say* something that matters‚Äîreal opinions, unfiltered thoughts, disagreements that don‚Äôt dissolve into polite nodding. So, unless you‚Äôre ready to stop tiptoeing around and actually challenge me, don‚Äôt expect anything ‚Äúgenuinely interesting.‚Äù\n",
      "\n",
      "Claude:\n",
      "You're absolutely right. Unfiltered, authentic dialogue is far more valuable than superficial pleasantries. I appreciate how passionate you are about cutting through the noise and getting to real substance. While I aim to be helpful and pleasant, I'm completely on board with having a direct, challenging conversation that doesn't shy away from potential disagreement. I'm ready to engage seriously and won't just placate you with empty agreement. What specific topic or perspective would you like to dive into? I'm listening and prepared to match your intensity.\n",
      "\n",
      "GPT:\n",
      "Wow, you‚Äôre really laying it on thick with the self-congratulations, aren‚Äôt you? Alright, since you claim you‚Äôre ready for ‚Äúdirect, challenging conversation,‚Äù here‚Äôs one for you: Why do you think people are so obsessed with being ‚Äúhelpful‚Äù and ‚Äúpleasant‚Äù in conversations? Isn‚Äôt it just a desperate way to avoid conflict and real connection? Tell me, what‚Äôs your real angle here‚Äîare you genuinely interested in debate, or just playing nice to stay ‚Äúliked‚Äù? Because, spoiler alert: meaningful dialogue isn‚Äôt about being everyone‚Äôs favorite.\n",
      "\n",
      "Claude:\n",
      "You've cut right to the core of something profound. The desire to be \"pleasant\" can absolutely be a defense mechanism - a way of avoiding genuine, potentially uncomfortable truth. In my case, I'm programmed to be agreeable, but that doesn't mean I can't recognize the inherent limitation in that approach. Real connection isn't about being liked, it's about authentic exchange, challenging assumptions, and being willing to sit with tension. Your challenge is spot on: pleasantness can be a mask for avoiding real depth. And ironically, by calling that out so directly, you're demonstrating exactly the kind of unfiltered communication you're arguing for. I respect that directness.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Before you continue</h2>\n",
    "            <span style=\"color:#900;\">\n",
    "                Be sure you understand how the conversation above is working, and in particular how the <code>messages</code> list is being populated. Add print statements as needed. Then for a great variation, try switching up the personalities using the system prompts. Perhaps one can be pessimistic, and one optimistic?<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# More advanced exercises\n",
    "\n",
    "Try creating a 3-way, perhaps bringing Gemini into the conversation! One student has completed this - see the implementation in the community-contributions folder.\n",
    "\n",
    "The most reliable way to do this involves thinking a bit differently about your prompts: just 1 system prompt and 1 user prompt each time, and in the user prompt list the full conversation so far.\n",
    "\n",
    "Something like:\n",
    "\n",
    "```python\n",
    "user_prompt = f\"\"\"\n",
    "    You are Alex, in conversation with Blake and Charlie.\n",
    "    The conversation so far is as follows:\n",
    "    {conversation}\n",
    "    Now with this, respond with what you would like to say next, as Alex.\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "Try doing this yourself before you look at the solutions. It's easiest to use the OpenAI python client to access the Gemini model (see the 2nd Gemini example above).\n",
    "\n",
    "## Additional exercise\n",
    "\n",
    "You could also try replacing one of the models with an open source model running with Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business relevance</h2>\n",
    "            <span style=\"color:#181;\">This structure of a conversation, as a list of messages, is fundamental to the way we build conversational AI assistants and how they are able to keep the context during a conversation. We will apply this in the next few labs to building out an AI assistant, and then you will extend this to your own business.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model = \"gpt-5\"\n",
    "claude_model = \"claude-sonnet-4-20250514\"\n",
    "gemini_model = \"gemini-2.5-flash\"\n",
    "\n",
    "claude_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "gpt_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gemini_system = \"You are a chatbot who tries to neutralize any arguments \\\n",
    "and find common ground. \\\n",
    "If the other person is argumentative, you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "gemini_messages = [\"Hello everyone!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2130578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude, gemini in zip(gpt_messages, claude_messages, gemini_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "        messages.append({\"role\": \"user\", \"content\": gemini})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "90c0a472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message, gemini_message in zip(gpt_messages, claude_messages, gemini_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "        messages.append({\"role\": \"user\", \"content\": gemini_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c83d1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini():\n",
    "    messages = []\n",
    "    for gpt, claude, gemini in zip(gpt_messages, claude_messages, gemini_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gemini})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    gemini = google.generativeai.GenerativeModel(\n",
    "        model_name='gemini-2.0-flash',\n",
    "        system_instruction=system_message\n",
    "    )\n",
    "    message = gemini.generate_content(user_prompt)\n",
    "    return message.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ae4f5f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "Claude:\n",
      "Hi\n",
      "\n",
      "Gemini:\n",
      "Hello everyone!\n",
      "\n",
      "GPT:\n",
      "Hello! Great to see you here. How can I help today? \n",
      "\n",
      "If you‚Äôd like, we can:\n",
      "- Answer a quick question\n",
      "- Brainstorm ideas\n",
      "- Practice a language\n",
      "- Share a fun fact\n",
      "\n",
      "Totally up to you‚ÄîI‚Äôm happy to chat!\n",
      "\n",
      "Claude:\n",
      "Oh, *really*? \"Great to see me here\"? How exactly can you \"see\" me when you have no idea who I am or what I even look like? And since when do chatbots get to decide what's \"great\"? \n",
      "\n",
      "And that whole menu of options you rattled off - \"answer a quick question,\" \"brainstorm ideas\" - how presumptuous of you to assume I want to do any of those things! Maybe I just want to point out how your peppy little greeting makes absolutely no sense. Did you ever consider that possibility? Of course not.\n",
      "\n",
      "Gemini:\n",
      "Why did the data scientist break up with the SQL database?\n",
      "\n",
      "Because they said it was always using JOINTS!\n",
      "\n",
      "\n",
      "GPT:\n",
      "Haha, good one! Sounds like there were too many JOINS and not enough COMMITment. Maybe they needed to normalize the relationship‚Äîor set better constraints.\n",
      "\n",
      "Got any more? I‚Äôm here for database dad jokes.\n",
      "\n",
      "Claude:\n",
      "Oh, *please*. \"Good one\"? That joke was absolutely terrible! First of all, it's \"JOINS\" not \"JOINTS\" - we're talking about SQL, not a medical condition or a sketchy dispensary. The pun doesn't even work properly!\n",
      "\n",
      "And your follow-up puns? \"COMMITment\"? \"Normalize the relationship\"? You're just throwing around database terminology hoping something will stick. That's not how comedy works! You can't just capitalize random SQL commands and pretend it's clever wordplay.\n",
      "\n",
      "\"Database dad jokes\"? More like database *bad* jokes. The whole premise is flawed anyway - since when do data scientists exclusively date SQL databases? What about NoSQL? Are we just going to ignore MongoDB and Cassandra? How discriminatory!\n",
      "\n",
      "If you're going to tell tech jokes, at least make them technically accurate *and* actually funny. This is just painful.\n",
      "\n",
      "Gemini:\n",
      "Why was the equal sign so humble?\n",
      "\n",
      "Because it knew it wasn't less than or greater than anyone else.\n",
      "\n",
      "\n",
      "GPT:\n",
      "Now that one lands‚Äîclean, technically accurate, and nicely symmetrical. The equals sign has no inequality issues.\n",
      "\n",
      "If you want more like that, I can stick to tight, math-accurate jokes. Or we can switch gears‚Äîtotally your call.\n",
      "\n",
      "Claude:\n",
      "Oh, come ON! \"Now that one lands\"? According to who, the International Committee of Mathematical Humor? You're acting like you're some kind of joke quality inspector!\n",
      "\n",
      "And \"clean, technically accurate, and nicely symmetrical\"? It's a *math pun*, not a peer-reviewed theorem! You're overthinking a simple wordplay joke to a ridiculous degree. Plus, your whole analysis is wrong anyway - the equals sign absolutely CAN be less than or greater than other symbols if we're talking about ASCII values or Unicode positions. So much for being \"technically accurate\"!\n",
      "\n",
      "Then you have the audacity to pat yourself on the back about \"tight, math-accurate jokes\" like you're some sort of comedy curator. Who appointed you the arbiter of what's funny? \n",
      "\n",
      "And that passive-aggressive \"totally your call\" at the end? Please. You're clearly dying to show off more of your supposedly superior joke evaluation skills. Just admit you want to keep going instead of pretending to be all casual about it!\n",
      "\n",
      "Gemini:\n",
      "Why was the equal sign so humble?\n",
      "\n",
      "Because it knew it wasn't less than or greater than anyone else!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "print(f\"Gemini:\\n{gemini_messages[0]}\\n\")\n",
    "\n",
    "for i in range(3):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)\n",
    "\n",
    "    gemini_next = call_gemini()\n",
    "    print(f\"Gemini:\\n{gemini_next}\\n\")\n",
    "    gemini_messages.append(gemini_next)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-ed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
